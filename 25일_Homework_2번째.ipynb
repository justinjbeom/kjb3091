{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "25일 Homework_R.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9I2gSbsO6g3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "outputId": "ff15ef0e-bd08-4751-d0fc-9a6ff544f70a"
      },
      "source": [
        "from __future__ import print_function           \n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "import time\n",
        "import os\n",
        "  \n",
        "# LeNet-Conv\n",
        "def lenet(x, y, name='lenet', reuse=None):\n",
        "  relu = tf.nn.relu\n",
        "  dense = tf.layers.dense\n",
        "  flatten = tf.contrib.layers.flatten\n",
        "\n",
        "  def conv(x, filters, kernel_size=3, strides=1, **kwargs):\n",
        "    return tf.layers.conv2d(x, filters, kernel_size, strides,\n",
        "        data_format='channels_first', **kwargs)\n",
        "\n",
        "  def pool(x, **kwargs):\n",
        "    return tf.layers.max_pooling2d(x, 2, 2,\n",
        "        data_format='channels_first', **kwargs)\n",
        "\n",
        "  def cross_entropy(logits, labels):\n",
        "    return tf.losses.softmax_cross_entropy(logits=logits,\n",
        "        onehot_labels=labels)\n",
        "\n",
        "  def accuracy(logits, labels):\n",
        "    correct = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
        "    return tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "\n",
        "# t의 Variable 정의  \n",
        "  t = tf.get_variable('temp', initializer=tf.constant(1.0))\n",
        "  x = tf.reshape(x, [-1, 1, 28, 28])\n",
        "  x = conv(x, 20, 5, name=name+'/conv1', reuse=reuse)\n",
        "  x = relu(x)\n",
        "  x = pool(x, name=name+'/pool1')\n",
        "  x = conv(x, 50, 5, name=name+'/conv2', reuse=reuse)\n",
        "  x = relu(x)\n",
        "  x = pool(x, name=name+'/pool2')\n",
        "  x = flatten(x)\n",
        "  x = dense(x, 500, activation=relu, name=name+'/dense', reuse=reuse)\n",
        "  logit = dense(x, 10, name=name+'/logits', reuse=reuse)\n",
        "# logit 정의 (공식에 따라서 t로 나눔)\n",
        "  logit = logit/t\n",
        "\n",
        "  net = {}\n",
        "  all_vars = tf.trainable_variables()\n",
        "  net['cent'] = cross_entropy(logit, y)\n",
        "  net['acc'] = accuracy(logit, y)\n",
        "  net['output'] = tf.nn.softmax(logit)\n",
        "  net['weights'] = [v for v in all_vars]\n",
        "# temperture variable 정의\n",
        "  net['temp_var'] = t\n",
        "  return net\n",
        "\n",
        "# MNIST data loader\n",
        "def mnist_input(path):\n",
        "    mnist = input_data.read_data_sets(path, one_hot=True, validation_size=0)\n",
        "    x, y = mnist.train.images, mnist.train.labels\n",
        "    y_ = np.argmax(y, axis=1)\n",
        "\n",
        "    xtr = [x[y_==k][:30,:] for k in range(10)]\n",
        "    ytr = [y[y_==k][:30,:] for k in range(10)]\n",
        "    xtr, ytr = np.concatenate(xtr, axis=0), np.concatenate(ytr, axis=0)\n",
        "\n",
        "    xva = [x[y_==k][30:40,:] for k in range(10)]\n",
        "    yva = [y[y_==k][30:40,:] for k in range(10)]\n",
        "    xva, yva = np.concatenate(xva, axis=0), np.concatenate(yva, axis=0)\n",
        "\n",
        "    xte, yte = mnist.test.images, mnist.test.labels\n",
        "    return xtr, ytr, xva, yva, xte, yte\n",
        "\n",
        "args = {\n",
        "    'mnist_path': './data',\n",
        "    'batch_size': 100,\n",
        "    'n_epochs': 200,\n",
        "    'gpu_num': 0    \n",
        "}\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = str(args['gpu_num'])\n",
        "                                         \n",
        "bs = args['batch_size']\n",
        "xtr, ytr, xva, yva, xte, yte = mnist_input(args['mnist_path'])\n",
        "x = tf.placeholder(tf.float32, [None, 784])\n",
        "y = tf.placeholder(tf.float32, [None, 10])\n",
        "net = lenet(x, y)\n",
        "loss = net['cent']\n",
        "  \n",
        "# Training\n",
        "def run():\n",
        "  global_step = tf.train.get_or_create_global_step()\n",
        "  lr_step = (300//args['batch_size'])*args['n_epochs']/2\n",
        "  lr = tf.train.piecewise_constant(tf.cast(global_step, tf.float32),\n",
        "      [lr_step], [1e-3, 1e-4])\n",
        "  train_op = tf.train.AdamOptimizer(lr).minimize(loss, \n",
        "                                                 global_step=global_step,\n",
        "                                                 var_list=net['weights'])\n",
        "\n",
        "  val_op = tf.train.AdamOptimizer(1e-1).minimize(loss,\n",
        "                                                 var_list=net['temp_var'])\n",
        "\n",
        "  sess = tf.Session()\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "\n",
        "  # Training\n",
        "  for i in range(args['n_epochs']):\n",
        "    # shuffle the training data every epoch\n",
        "    xytr = np.concatenate((xtr, ytr), axis=1)\n",
        "    np.random.shuffle(xytr)\n",
        "    xtr_, ytr_ = xytr[:,:784], xytr[:,784:]\n",
        "\n",
        "    for j in range(300//args['batch_size']):\n",
        "      bx, by = xtr_[j*bs:(j+1)*bs,:], ytr_[j*bs:(j+1)*bs,:]\n",
        "      _, cent, acc = sess.run([train_op, net['cent'], net['acc']], \n",
        "                              {x:bx, y:by})    \n",
        "    if i % 10 == 0:\n",
        "      print('epoch %d: cent = %f, acc = %f' % (i, cent, acc))\n",
        "\n",
        "# temperature 적용 전 ECE 산출 출력\n",
        "  # Test & ECE\n",
        "  cent, acc, output = sess.run([net['cent'], net['acc'], net['output']], \n",
        "                               {x:xte, y:yte})\n",
        "  label = yte\n",
        "  print('Test: cent=%f, acc=%f' % (cent, acc))\n",
        "  print('ECE = %f' % ece(output, label))      \n",
        "      \n",
        "  # temperature scaling\n",
        "  for i in range(args['n_epochs']):\n",
        "    _, cent, acc = sess.run([val_op, net['cent'], net['acc']], {x:xva, y:yva})\n",
        "    if i % 10 == 0:\n",
        "      print('epoch %d: cent = %f, acc = %f' % (i, cent, acc))\n",
        "\n",
        "  # Test & ECE\n",
        "  cent, acc, output = sess.run([net['cent'], net['acc'], net['output']], \n",
        "                               {x:xte, y:yte})\n",
        "  label = yte\n",
        "  print('Test: cent=%f, acc=%f' % (cent, acc))\n",
        "  print('ECE = %f' % ece(output, label))\n",
        "  \n",
        "def ece(output, label):\n",
        "  idx = (np.arange(10000),np.argmax(output,1))\n",
        "  conf = output[idx]\n",
        "  correct = label[idx]\n",
        "\n",
        "  M = 10\n",
        "  bins, confs, accs = np.zeros(M), np.zeros(M), np.zeros(M)\n",
        "\n",
        "  for m in range(M):\n",
        "    idx = (m*0.1 <= conf) * (conf <= (m+1)*0.1)\n",
        "    nbin = sum(idx)\n",
        "    bins[m] = nbin\n",
        "    confs[m] = 0. if nbin == 0 else conf[idx].mean()\n",
        "    accs[m] = 0. if nbin == 0 else correct[idx].mean()\n",
        "\n",
        "  ece = np.sum((bins/float(10000))*np.abs(accs-confs))\n",
        "  return ece"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0731 13:35:46.215948 139848038320000 deprecation.py:323] From <ipython-input-1-8a750d953759>:57: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "W0731 13:35:46.217609 139848038320000 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "W0731 13:35:46.219135 139848038320000 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "W0731 13:35:46.318069 139848038320000 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "Extracting ./data/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0731 13:35:46.611317 139848038320000 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "W0731 13:35:46.615069 139848038320000 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "W0731 13:35:46.717797 139848038320000 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "Extracting ./data/train-labels-idx1-ubyte.gz\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting ./data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting ./data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0731 13:35:47.132567 139848038320000 deprecation.py:323] From <ipython-input-1-8a750d953759>:16: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2D` instead.\n",
            "W0731 13:35:47.138440 139848038320000 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0731 13:35:47.408871 139848038320000 deprecation.py:323] From <ipython-input-1-8a750d953759>:20: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.MaxPooling2D instead.\n",
            "W0731 13:35:47.575707 139848038320000 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "W0731 13:35:47.945828 139848038320000 deprecation.py:323] From <ipython-input-1-8a750d953759>:40: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "W0731 13:35:48.352365 139848038320000 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owQUwI9zPavJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        },
        "outputId": "703f71f6-8a3d-4329-aa15-6491e613a05a"
      },
      "source": [
        "run()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0: cent = 2.200900, acc = 0.290000\n",
            "epoch 10: cent = 0.267140, acc = 0.890000\n",
            "epoch 20: cent = 0.042162, acc = 1.000000\n",
            "epoch 30: cent = 0.006216, acc = 1.000000\n",
            "epoch 40: cent = 0.001656, acc = 1.000000\n",
            "epoch 50: cent = 0.001078, acc = 1.000000\n",
            "epoch 60: cent = 0.001350, acc = 1.000000\n",
            "epoch 70: cent = 0.000759, acc = 1.000000\n",
            "epoch 80: cent = 0.000561, acc = 1.000000\n",
            "epoch 90: cent = 0.000410, acc = 1.000000\n",
            "epoch 100: cent = 0.000352, acc = 1.000000\n",
            "epoch 110: cent = 0.000302, acc = 1.000000\n",
            "epoch 120: cent = 0.000268, acc = 1.000000\n",
            "epoch 130: cent = 0.000197, acc = 1.000000\n",
            "epoch 140: cent = 0.000314, acc = 1.000000\n",
            "epoch 150: cent = 0.000377, acc = 1.000000\n",
            "epoch 160: cent = 0.000345, acc = 1.000000\n",
            "epoch 170: cent = 0.000244, acc = 1.000000\n",
            "epoch 180: cent = 0.000252, acc = 1.000000\n",
            "epoch 190: cent = 0.000299, acc = 1.000000\n",
            "Test: cent=0.595581, acc=0.880600\n",
            "ECE = 0.077719\n",
            "epoch 0: cent = 0.430396, acc = 0.940000\n",
            "epoch 10: cent = 0.281183, acc = 0.940000\n",
            "epoch 20: cent = 0.271825, acc = 0.940000\n",
            "epoch 30: cent = 0.274138, acc = 0.940000\n",
            "epoch 40: cent = 0.272910, acc = 0.940000\n",
            "epoch 50: cent = 0.271547, acc = 0.940000\n",
            "epoch 60: cent = 0.271453, acc = 0.940000\n",
            "epoch 70: cent = 0.271485, acc = 0.940000\n",
            "epoch 80: cent = 0.271420, acc = 0.940000\n",
            "epoch 90: cent = 0.271415, acc = 0.940000\n",
            "epoch 100: cent = 0.271415, acc = 0.940000\n",
            "epoch 110: cent = 0.271412, acc = 0.940000\n",
            "epoch 120: cent = 0.271412, acc = 0.940000\n",
            "epoch 130: cent = 0.271412, acc = 0.940000\n",
            "epoch 140: cent = 0.271412, acc = 0.940000\n",
            "epoch 150: cent = 0.271412, acc = 0.940000\n",
            "epoch 160: cent = 0.271412, acc = 0.940000\n",
            "epoch 170: cent = 0.271412, acc = 0.940000\n",
            "epoch 180: cent = 0.271412, acc = 0.940000\n",
            "epoch 190: cent = 0.271412, acc = 0.940000\n",
            "Test: cent=0.383036, acc=0.880600\n",
            "ECE = 0.011501\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}